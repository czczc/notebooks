# Bridging Simulation-to-Data Gaps in High-Energy Physics

**Introduction.** Machine learning models in high-energy physics (HEP) are typically trained on Monte Carlo simulations (which encode our best understanding of physics processes and detector responses) and then applied to real experimental data. Any *domain shift* – systematic differences between simulated and real data – can severely degrade model performance. In practice, **ML models trained on simulation often struggle on experimental data** because of unmodeled effects like detector noise, calibration drifts, or physics approximations. This issue spans collider experiments (e.g. jets, particle tracking), neutrino detectors, astrophysics surveys, and more. Researchers have developed a broad toolbox to handle simulation-to-data discrepancies. Below we present a taxonomy of these methods, with use cases, strengths, and limitations. Key techniques include *distribution reweighting*, *domain adaptation (often via adversarial training)*, *simulation-based inference*, and emerging approaches like *transfer learning*, *contrastive/self-supervised learning*, and *foundation models*. Tables 1 and 2 summarize major methods.

## Taxonomy of Methods for Simulation-to-Data Discrepancies

**1. Distribution Reweighting and Calibration.** A classical approach is to reweight simulated events so that certain feature distributions match those in real data. By assigning a weight \$w(x)\$ to each simulated event (based on its features \$x\$), one can morph the simulation distribution into the target distribution. This can correct for known differences (e.g. particle \$p\_T\$ spectra, pile-up conditions) without retraining models. Traditional reweighting uses physics knowledge or low-dimensional fits, but **machine-learning-based reweighting** methods are now common. For example, the CMS experiment has demonstrated reweighting of Monte Carlo samples using boosted decision trees and deep neural networks to emulate *alternative generator settings* or *different physics parameters*. By training a network to distinguish a nominal simulation from a modified simulation, one can derive event weights that *make the nominal sample imitate the alternative* (the DCTR method). Another example is **SALAD (Simulation Assisted Likelihood-Free Anomaly Detection)**, which learns a *parameterized reweighting function* to morph simulated background to match real data in sideband regions. The learned weights adjust the simulation so that its feature distributions align with data, enabling more reliable use of simulation in signal regions. Reweighting is widely used for data-driven *calibration*: e.g. ATLAS/CMS apply data-derived scale factors or weights so that simulated and observed distributions (of lepton efficiencies, particle multiplicities, etc.) agree within uncertainties.

> **Example – Top Quark Simulations:** CMS demonstrated ML reweighting to account for different generator settings in top-quark pair production. A neural network was trained to reweight events from a nominal generator tune to mimic an alternative tune, avoiding the need to produce large simulated samples for each variation. The reweighted nominal sample reproduced the kinematic distributions of the alternative simulation, reducing statistical uncertainties and computational cost.

> **Strengths:** Reweighting is transparent and leverages known physics differences. It preserves the original simulation (only adjusting event importance) and can incorporate high-dimensional information via ML. It’s especially powerful when differences are understood (e.g. a specific parameter shift) or when a small data control sample can guide the weights (as in SALAD for background modeling).

> **Limitations:** Reweighting assumes the simulated feature space overlaps with data – it cannot correct for *entirely novel features or detector effects* absent in simulation. Extreme reweights (large \$w\$) can reduce effective statistics. Moreover, reweighting typically focuses on marginal distributions; mismatches in complex joint distributions or unseen regions remain challenging. Care must be taken to avoid biasing the physics content when applying weights.

**2. Domain Adaptation and Domain-Invariant Learning.** **Domain adaptation** techniques seek to learn representations that are *independent of the domain (simulation vs. real)* so that models generalize despite simulation–data differences. A common approach is the **Domain-Adversarial Neural Network (DANN)**, which introduces an auxiliary adversarial task to encourage domain-indistinguishable features. In a DANN, a feature extractor is trained to minimize the primary task loss (e.g. classification) *and simultaneously* to *confuse* a domain discriminator that tries to classify whether a sample comes from simulation or real data. By reversing the gradient from the domain discriminator, the feature encoder learns a *domain-invariant latent space*. This was demonstrated in the ALICE experiment’s particle identification: models trained only on simulation gave poor results on real detector data, but adding adversarial domain adaptation enabled the network to **find common features between simulation and production data**, improving real-data accuracy. Similarly, Baalouch *et al.* (LHCb Flavours of Physics Kaggle challenge) applied a DANN to ensure a classifier performed consistently on simulated *and* real collision data. The DANN reduced the discrepancy between domains (measured by Kolmogorov–Smirnov tests on classifier outputs), indicating successful sim-to-real transfer. Domain adaptation has also shown promise in adjacent fields – for instance, in astrophysics, where models classifying ionized nebulae failed due to differences between theoretical photoionization models and observed spectra. Using a DANN to *bridge the gap* between simulated and observed nebula spectra yielded a **24% increase in classification accuracy on real data** compared to a simulation-trained classifier. This highlights how domain-invariant feature learning can markedly improve robustness on the target domain.

> **Example – Neutrino Detector Domain Shift:** Liquid Argon TPC neutrino experiments (MicroBooNE) encountered domain shift because simulated images lacked realistic noise/artifact patterns. Researchers explored *unpaired image-to-image translation* (CycleGAN-based domain adaptation) to convert between simulated and real detector images. By mapping real data into the simulation domain (and vice versa), they could run pattern-recognition algorithms with much less performance degradation. This domain translation approach improved downstream neutrino event reconstruction without requiring labels, effectively **mitigating LArTPC detector effects** through unsupervised learning.

> **Strengths:** Domain adaptation methods exploit *unlabeled* real data (abundant in physics) alongside labeled simulation. They correct for *unknown or high-dimensional discrepancies* by learning representations that filter out simulator-specific quirks. Adversarial approaches are flexible – they can incorporate multiple domains or even continuous domain variables. Successes in LHC experiments, ALICE, and astrophysics demonstrate significantly improved consistency between simulation and data. Notably, these techniques don’t require *a priori* knowledge of the mismatch sources.

> **Limitations:** Adversarial training can be unstable and requires careful hyperparameter tuning (to balance task performance vs. domain confusion). If simulation and data differ in fundamental ways (e.g. completely missing physics in simulation), domain-invariant features might ignore important information (potentially under-fitting the signal). There’s a trade-off: pushing for complete domain invariance may **decrease accuracy** on the primary task if domain-specific features were informative. Additionally, diagnosing *what* the model has learned to ignore can be non-trivial – one must ensure that only nuisance differences (not genuine physics signals) are being factored out.

**3. Adversarial Training for Systematic Invariance.** A related class of techniques uses adversarial objectives to *remove sensitivity to known systematic differences*. Instead of treating “simulation vs. data” as a binary domain label, HEP often has *continuous nuisance parameters* (e.g. an uncertainty in calorimeter energy scale or theoretical model variation). **Learning to Pivot** with adversarial networks (Louppe *et al.* 2017) introduced a framework to enforce that a model’s predictions are independent of a given nuisance parameter. In effect, the model becomes “blind” to that systematic variation – a desirable property termed *the pivotal property*. For example, one can train a jet tagging classifier with an adversary that tries to infer whether an event was generated with nominal or varied simulation settings (such as different parton shower tunes). The classifier is penalized if the adversary can tell the difference, thereby **making the classifier’s internal representation insensitive to those simulation uncertainties**. This improves robustness: the classifier should then perform consistently even if the true data correspond to a slightly different generator tune or calibration than the nominal simulation. Adversarial “decorrelation” has been applied in LHC searches to ensure that a classification score is independent of certain kinematic variables (often those where data/simulation disagreement is observed), thus reducing bias when selecting events.

> **Example – Mass-Decorrelated Tagging:** In resonance searches, a common worry is that a classifier might learn the subtle differences between simulated and real data in a control region (e.g. slight mismatch in dijet mass shape). Using an adversarial network that forces the classifier output to be independent of the dijet mass, physicists achieved a classifier that didn’t sculpt the mass distribution. This adversarially-trained model was far less sensitive to the exact shape differences between simulation and data, making the search more robust against background mismodeling.

> **Strengths:** Adversarial invariance training directly addresses *known systematics* by incorporating them into the training objective. It provides a knob to tune the **accuracy–robustness trade-off**, allowing physicists to enforce as much invariance as needed to stay within systematic uncertainties. This approach can significantly reduce the impact of specific simulation deficiencies (e.g. mis-modeled detector response) on ML outputs, instilling confidence that model decisions rely only on truly physical features.

> **Limitations:** Each adversarial target addresses one aspect of domain shift at a time – it requires identifying which variable or parameter to pivot on. If a discrepancy is not explicitly targeted, it may not be mitigated. Moreover, using adversaries on continuous parameters assumes one can parameterize the family of simulations; unknown unknowns remain problematic. There is also an **accuracy cost**: making a classifier completely agnostic to a variable can degrade its discrimination power if that variable was correlated with the label. The challenge is choosing which features to make invariant such that we remove spurious domain differences while retaining genuine signal sensitivity.

**4. Simulation-Based Inference (Likelihood-Free Inference).** Unlike the above strategies – which focus on improving predictive models – **simulation-based inference (SBI)** techniques fundamentally recast the problem: instead of training a model to predict event labels, one uses the simulator *within an inference algorithm* to directly estimate physics parameters from data. SBI (also called *likelihood-free inference*) acknowledges that writing down an analytical likelihood for high-dimensional data is infeasible, but forward simulation is possible. By clever use of machine learning, one can **fit physics parameters by comparing simulated and observed data without an explicit likelihood function**. A seminal example is the **MadMiner** library, which automates optimal multivariate inference for LHC measurements. It uses matrix-element information and ML to compute likelihood ratios or score functions from simulated events, without needing to simplify the data or detector effects. The advantage in the context of domain shift is that **the full simulation is used “as-is”** – there is no intermediate ML model being deployed on data that could fail if simulation is imperfect. Instead, SBI methods adjust the *simulator parameters (including nuisance parameters)* to best fit the real data. In doing so, they effectively “learn” how data differ from nominal simulation by finding the parameter values (e.g. calibration factors, cross-section tweaks) that bring simulation and data into agreement. For instance, recent HEP applications perform neural likelihood ratio estimation to constrain parton distribution functions or detector calibration parameters by maximizing agreement between simulated and observed distributions. SBI also includes approaches like Approximate Bayesian Computation and normalizing flows to directly *sample* the posterior of physics parameters given observed data and a generative simulator.

> **Example – Effective Field Theory Fits:** In measurements of Higgs boson interactions, many subtle kinematic effects are sensitive to BSM physics. Rather than comparing a few histograms, ATLAS/CMS have explored SBI tools (e.g. neural score estimation) to fit Wilson coefficients of EFT operators. By leveraging *all dimensions* of the event data through the simulator, these methods attained tighter constraints than traditional analysis, **without requiring data reduction to a single variable or a simplified detector model**. The ML essentially learns an optimal test statistic from simulation that accounts for detector smearing and accepts/rejects, ensuring maximal use of information.

> **Strengths:** Simulation-based inference makes *maximal use of the simulation’s fidelity*. It sidesteps the need to train a discriminative model that might extrapolate poorly – instead, the comparison always occurs via simulated pseudo-data. Modern SBI techniques can be multi-dimensional and multivariate, far exceeding the old paradigm of tuning a single summary statistic. They naturally incorporate uncertainties by treating them as latent parameters to marginalize or profile. Notably, SBI approaches **avoid hard-coding summary cuts or likelihood assumptions**, thus no information is unnecessarily thrown away. If the simulator is accurate (or can be calibrated via included nuisance parameters), SBI yields unbiased, high-precision results.

> **Limitations:** SBI is computationally intensive – it may require thousands to millions of simulator runs and training neural networks to represent likelihood ratios or posteriors. If the simulator is missing an effect (say, unknown physics or detector issues), SBI will not magically fix it – one can only infer within the model’s assumptions. Thus, a poor simulator still leads to biased inferences; SBI just helps extract all possible info given the model. Additionally, SBI methods can be complex to validate, and their adoption in large collaborations is ongoing. They often demand expertise in both ML and domain physics to set up correctly. Finally, unlike a simple reweight or calibration factor, the results of SBI (e.g. a learned likelihood ratio function) may be harder to interpret physically, so careful cross-checks with simpler methods are usually needed.

**Table 1 – Established Techniques for Simulation-to-Data Domain Shift** (HEP = high-energy physics, MC = Monte Carlo, DA = domain adaptation, *adv* = adversarial)

| **Method**                                                         | **Example Use (Physics)**                                                                                                                         | **Strengths**                                                                                                                                                                                                        | **Limitations**                                                                                                                                                                              |
| ------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Reweighting** (adjust sim. event weights to data)                | ML reweighting in CMS for top-quark MC tuning; SALAD anomaly detection reweighting LHC jets.                                                      | Straightforward *post hoc* calibration; uses known features; ML allows high-dimensional matching.                                                                                                                    | Struggles if sim. lacks features present in data; can require large statistics; extreme weights add noise.                                                                                   |
| **Domain Adaptation** (learn domain-invariant features)            | Adversarial DANN for ALICE particle ID; LHCb Kaggle sim-to-real classifier; DANN for astrophysics nebula spectra.                                 | Uses unlabeled real data to improve generalization; addresses unknown multi-dimensional shifts; shown to significantly improve real-data performance (e.g. +24% accuracy).                                           | Adversarial training is delicate (stability, hyperparams); risk of underfitting if domains differ too much; might remove informative features if mis-configured.                             |
| **Adversarial Invariance** (remove dependence on known nuisance)   | “Learning to Pivot” – make classifier output independent of simulation systematics; decorrelating jet tagger from mass or calibration variations. | Robustifies models against specific, identified discrepancies; tunable trade-off between accuracy and robustness; improves reliability for measurements.                                                             | Requires *a priori* knowledge of which variable or parameter to make invariant; cannot address unknown biases; slight performance loss on main task is common.                               |
| **Simulation-Based Inference** (fit parameters via simulator & ML) | MadMiner for LHC EFT fits (uses full event info); neutrino interaction model calibration with likelihood-free methods.                            | Uses full physics and detector simulation – no need to simplify data; can tune simulation parameters to data, effectively “learning” the data–MC differences; provides statistically optimal inference in high dims. | Computationally heavy (many MC runs, complex ML training); only as good as the simulator’s completeness; results can be opaque and need careful validation; not yet routine in all analyses. |

## Emerging and Cross-Disciplinary Techniques

Beyond the established toolkit, HEP is beginning to explore modern machine learning paradigms that have transformed other domains. These techniques – **transfer learning, contrastive/self-supervised learning, and large “foundation” models** – hold promise for addressing sim-to-real gaps, even if they are not yet common practice in physics. We describe these below, including early examples and potential benefits.

**5. Transfer Learning and Fine-Tuning.** In situations where a modest amount of *real data with labels* is available (or data from a related experiment), transfer learning can dramatically improve model performance on the target domain. The idea is to **pretrain a model on large simulated datasets, then fine-tune on a smaller real-data sample**, so the model adapts to real distributions. This approach is widely successful in computer vision (e.g. fine-tuning ImageNet models) and is gaining traction in HEP. For instance, a recent study on particle-flow reconstruction showed that a graph neural network trained on one detector’s full simulation could be *fine-tuned* to a new detector geometry using 10× less data, yet achieve the same performance as training from scratch on the new detector. In fact, the fine-tuned model reached comparable performance to the traditional (non-ML) algorithm with only \~100k real events, whereas a fresh ML model needed ∼1 million events to converge. This **cross-detector transfer** highlights how knowledge learned in simulation can be repurposed efficiently for a different domain. In an experimental context, one might train a model on simulation and then update it with a *calibration dataset* (e.g. real particle signals that can be identified through special triggers or well-known decays). Even without explicit labels, unsupervised fine-tuning (e.g. using reconstruction losses on real data) can adjust a model to the new domain.

> **Example – Cosmic-Ray Fine-Tuning:** The NOvA neutrino experiment used transfer learning to improve cosmic-ray background rejection. A convolutional network was first trained on simulation to identify particle showers. Then, using a small set of hand-scanned real events, the model was fine-tuned. The result was a notable reduction in domain-induced error, as the network learned real detector noise characteristics that were absent in simulation. This reduced false positives when analyzing actual data.

> **Strengths:** Transfer learning leverages the *abundant simulated data* to learn general high-level features, then only needs a fraction of real data to calibrate domain-specific low-level differences. It **greatly boosts data efficiency** – as shown in the particle-flow study, an order of magnitude less data was sufficient with fine-tuning. It is relatively easy to implement (fine-tuning a pre-trained model is standard in ML frameworks) and can be combined with any of the above methods (one could fine-tune a DANN-adapted model, for example). Moreover, transfer learning opens the door to *cross-experiment generalization*: as HEP moves toward shared ML models, one can imagine pretraining on simulation for one experiment and adapting to another, saving training time and resources.

> **Limitations:** The obvious requirement is having some target-domain data to fine-tune on – which in cases of completely new physics signals or very limited data might be challenging. If the simulation and data are too different, fine-tuning might still struggle or could overfit the small real sample. Careful regularization and freeze-or-tune decisions for various layers are needed (e.g. low-level feature layers might need more adjustment if the detector technologies differ significantly). In addition, transfer learning does not *eliminate* domain shift by itself; it reduces it by explicitly training on the new domain. If mislabeled or biased data are used in fine-tuning, they could actually introduce new systematics. Finally, not all physics tasks have a clear source of “partial labels” in real data to exploit – sometimes unsupervised domain adaptation (as above) is the only option if truly no labels exist.

**6. Self-Supervised and Contrastive Learning.** In industry, self-supervised learning (SSL) has enabled large models to learn from unlabeled data by formulating proxy tasks (e.g. predicting masked pixels or solving jigsaw puzzles in images). In HEP, a growing research direction is to use **physics-inspired augmentations** and contrastive objectives to learn representations that capture underlying physics features while being less sensitive to domain specifics. **Contrastive learning** encourages representations where *augmented views of the same event* are pulled together in latent space, while different events are pushed apart. Wilkinson *et al.* (2025) applied contrastive learning to **neutrino detector data**, using simulated events. They generated controlled augmentations of each simulated detector event (e.g. adding noise, slightly altering energy deposit patterns within plausible ranges) and trained a network to recognize these as the “same” underlying event. The result was a representation that is *robust and transferable*: the network learned features that correlated with the true physics of the neutrino interactions rather than simulation-specific details. The authors found this improved generalization to real data and noted unique advantages of contrastive SSL over standard domain adaptation. Another exciting development is **Resimulation-Based Self-Supervised Learning (RS3L)**. RS3L takes advantage of *stochastic simulators* by “re-simulating” the same physics event multiple times: for example, run a particle shower simulation with different random seeds or alternate physics models. By intervening mid-simulation (say, at the hard-scattering level) and re-running the downstream detector simulation, RS3L produces a set of *different outcomes for the same true event*. These serve as natural augmentations for contrastive learning – the network must learn the *common essence* of an event across simulation variations. Applied to jet physics, RS3L learned an 8-dimensional latent space where jets from two different generator codes (PYTHIA vs. HERWIG) mapped to similar representations. In effect, the representation became **less dependent on the specific simulation software**, focusing on physical jet properties. Models pretrained with RS3L on millions of simulated jets showed improved performance on several downstream tasks (jet tagging, uncertainty estimation), indicating the promise of self-supervised pretraining in HEP.

> **Example – Contrastive Representations for Muon Tracking:** Consider a muon detector where real data suffer from occasional strip misfires not present in simulation. Using contrastive learning, one could take a simulated muon track and apply artificial “glitches” (drop some hits, add noise) to create augmented views. A network trained to identify tracks invariantly – whether or not glitches are present – will likely learn to focus on robust track features like geometry and timing. Such a representation, once trained on simulation plus augmentations, could be applied to real data where similar glitches occur, handling them gracefully because it has been trained to not over-rely on pristine simulation details.

> **Strengths:** Self-supervised and contrastive methods can exploit vast unlabeled datasets (including real experimental data if available, by pairing real-real augmentations) to learn high-quality features. They **do not require explicit domain labels or class labels**, yet can yield representations that naturally align simulation with data. By appropriate choice of augmentations (guided by physics knowledge), one can encode invariances that correspond to detector distortions or calibration shifts, thereby pretraining a model to be agnostic to those differences. The neutrino study showed contrastive learning can outperform classical domain adaptation in some cases, as it forces the network to *learn the signal in the noise*. SSL-pretrained models also serve as a starting point for many tasks, potentially reducing the need for task-specific simulation samples.

> **Limitations:** Designing effective augmentations is non-trivial – too simplistic, and the model learns nothing new; too aggressive, and it might learn to ignore important subtle features. In physics, augmentations must be physically plausible (e.g. one wouldn’t randomly permute calorimeter cell energies, as that would break physical correlations). There is a risk of the model learning to compress irrelevant simulation artifacts if augmentations are insufficient. Also, SSL usually requires very large models/data to shine (e.g. millions of events, high-capacity networks), so the compute burden can be high. We are only beginning to see how these techniques apply to HEP – best practices are not yet established. Finally, evaluating the quality of learned representations requires downstream benchmarks; it’s not always obvious how to interpret the latent features (though physics-motivated probes, as done in RS3L, can help).

**7. Foundation Models and Large-Scale Pretraining.** Tying together many of the above trends is the vision of **foundation models** for science – analogous to large language models or vision transformers in other fields. In HEP, a foundation model would be a *universal*, pre-trained model (or representation) that spans multiple datasets, tasks, and even experiments. Once trained on a broad set of simulation data (and possibly unsupervised real data), such a model could be fine-tuned quickly for specific analysis tasks, benefiting from both cross-domain knowledge and massive training corpora. Early steps toward this include **OmniJet-α**, a transformer-based model trained on a large collection of jet simulation tasks. OmniJet’s novelty is demonstrating **cross-task transfer**: it learned a generative model of particle jets (unsupervised density estimation) and this knowledge was successfully transferred to a supervised jet tagging task. This marked the **first time** a single model showed strong performance on two very different jet problems, a milestone for foundation models in particle physics. Another example is “Bumblebee,” a BERT-like model for collider events that is being trained to identify novel physics signals across many background processes (inspired by natural language foundation models). The **Fine-Tuning Particle Flow (FTPF)** study (Mokhtar *et al.* 2025) explicitly connects transfer learning to foundation models, suggesting that their cross-detector particle-flow network could be a stepping stone to a large model applicable to any detector geometry. In astrophysics, researchers talk of foundation models for galaxy images that, once pre-trained on simulation and survey data, could adapt to new surveys or tasks with minimal effort. The motivation is clear: if successful, foundation models could **improve physics performance and drastically reduce the required training data and time** for each new application. They would inherently handle some degree of domain shift because they are trained on *multiple domains and tasks at once*, learning very general representations.

> **Example – Multi-Experiment Particle ID:** Envision a graph neural network trained on a *union* of LHC, neutrino, and cosmic-ray detector simulations – different detectors and particle types, with a self-supervised objective to predict missing sensor readings (a kind of physics-inspired masked modeling). The resulting model could act as a universal particle identification engine. When a new experiment comes online, one would only need to fine-tune this foundation model on a small calibration dataset (transfer learning) to achieve good performance, rather than starting from scratch. Because the model has “seen” many detector domains in its pre-training, it is naturally more robust to moderate domain shifts.

> **Strengths:** If achieved, foundation models would provide *unprecedented flexibility*: a single pre-trained model could be deployed across analyses, saving enormous simulation and training efforts. Their multi-task nature means they **learn from diverse data** – effectively performing an in situ domain adaptation across different simulation conditions, detectors, and physics processes. This breadth should make them less susceptible to any one simulation’s biases. They also encourage *standardization* of ML approaches in HEP, potentially capturing community knowledge of how to best represent physics data.

> **Limitations:** Training such models is an immense undertaking – requiring large computational resources and careful curation of training data from various sources. There is a risk of “forgetting” important experiment-specific details if the model over-generalizes. Moreover, evaluating a foundation model’s performance on all relevant tasks (and domains) is complex; it might do well on average but could underperform on a niche corner-case that a specialized model would handle. The interpretability of these giant models is also a concern, especially in science where understanding *why* a model made a decision can be as important as the decision itself. Finally, this field is very new – OmniJet and others are pioneering results, but it remains to be seen how much practical impact they will have on real experimental analyses in the next few years.

**Table 2 – Emerging Techniques from Broader ML** (being explored in HEP for sim-to-real challenges)

| **Emerging Technique**                                                    | **Current/Potential Use**                                                                                                                                             | **Prospects**                                                                                                                                                                                                                                          | **Challenges**                                                                                                                                                                                                                                                               |
| ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Transfer Learning** (pretrain on sim, fine-tune on data)                | Cross-detector GNN fine-tuning for particle flow (CLIC ➜ FCC); calibrating sim-trained models with real data samples.                                                 | Greatly reduces required data for new domain (10× less in PF study); quickly adapts models to experiment-specific conditions; can combine with any method above.                                                                                       | Needs some labeled (or pseudo-labeled) real data; risk of overfitting small fine-tune set; large sim–data discrepancy can still hurt if fine-tune is insufficient.                                                                                                           |
| **Self-Supervised & Contrastive** (learn physics features without labels) | Contrastive augments for neutrino events; RS3L on jet simulations for domain-agnostic jet features; unlabeled LHC data for pretraining event encoders.                | Leverages vast unlabeled data to learn robust, transferable representations; can encode invariances to detector effects via data augmentations; improves generalization to real data by focusing on underlying physics signals.                        | Requires careful design of augmentations (physics knowledge); very large models/data often needed for full benefit; still experimental – integration into analysis pipelines is just beginning; difficult to quantify what features are learned without thorough validation. |
| **Foundation Models** (large pre-trained models spanning tasks)           | OmniJet-α transformer for jets (learned gen + tagging); “Bumblebee” BERT model for event anomaly detection; vision models for cosmology trained on multi-survey data. | **Multi-domain, multi-task training** can produce a single model adaptable to many analyses; promises state-of-the-art performance with minimal fine-tuning per task; naturally incorporates a degree of domain adaptation by training on varied data. | Enormous training cost and complexity; not yet proven in critical physics measurements; risk of over-generalization or missing experiment-specific details; challenges in collaboration adoption and validation (black-box concerns).                                        |

## Conclusion

High-energy physics is actively developing strategies to ensure that machine learning models trained on simulation remain reliable when faced with real experimental data. The *well-established methods* – such as reweighting simulated events, adversarial domain adaptation, and embedding simulation uncertainties into training – are already improving the robustness of physics analyses. At the same time, *new techniques* from the wider ML community are making inroads: contrastive learning can extract truly physical signals from simulated data that carry over to real detectors, and large-scale pretraining hints at a future where a single foundation model might underpin many physics tasks. No single method is a panacea for the simulation-to-data gap – each comes with strengths and caveats (Tables 1 and 2). In practice, physicists often combine approaches: for example, using reweighting to correct coarse differences, adversarial training to handle known systematics, and domain adaptation to fine-tune the model with control-region data. The encouraging message is that *the synergy of physics insights and modern ML* is yielding a powerful arsenal to tackle domain shift. As these techniques mature, we expect more reliable ML applications in collider physics, neutrino experiments, and astrophysics, ultimately enhancing the discovery potential and precision of HEP experiments in the face of ever-more complex data.

**References:** Key resources include Baalouch *et al.* (2019) on sim-to-real transfer with DANN, CMS Collaboration (2025) on ML-based event reweighting, Andreassen, Nachman & Shih (2020) on SALAD reweighting for anomaly detection, Louppe *et al.* (2017) on adversarial pivoting, Brehmer *et al.* (2019) on MadMiner for likelihood-free inference, Wilkinson *et al.* (2025) on contrastive learning for neutrinos, Harris *et al.* (2025) on RS3L self-supervised pretraining, and Birk *et al.* (2024) on the OmniJet foundation model. These and the works cited throughout illustrate the community’s multi-pronged approach to bridging the sim-to-real divide – a crucial step as we rely on ML for the next generation of physics discoveries.
